# -*- coding: utf-8 -*-
"""ASL_MediaPipe_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eUWdK83BOxQBLdS4Se6ODZb3b0luABQc

# ASL Alphabet Recognition using MediaPipe Landmarks
Train a simple model on the ASL Alphabet dataset using MediaPipe hand landmarks instead of raw images.
"""

from google.colab import drive
drive.mount('/content/drive')

!cp "/content/drive/MyDrive/asl_alphabet_train.rar" "/content/"
!unrar x "/content/asl_alphabet_train.rar" "/content/asl_dataset/" -o+

# Force install working versions
!pip install --upgrade --force-reinstall \
    numpy==1.23.5 \
    protobuf==3.20.* \
    opencv-python-headless==4.7.0.72 \
    mediapipe==0.10.9

# Restart runtime after this

!pip install numpy==1.26.4 protobuf==4.25.7 mediapipe==0.10.21

import numpy as np
print("NumPy version:", np.__version__)

import mediapipe as mp
print("MediaPipe imported successfully!")

import os
import cv2
import numpy as np
import mediapipe as mp
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras import layers, models

# Mount Google Drive if your dataset is there
# from google.colab import drive
# drive.mount('/content/drive')

# Define dataset path (adjust this as needed)
DATA_DIR = "/content/asl_dataset/asl_alphabet_train/asl_alphabet_train"
CATEGORIES = sorted(os.listdir(DATA_DIR))
print("Classes:", CATEGORIES)

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1)
mp_drawing = mp.solutions.drawing_utils

data = []
labels = []

for label_index, category in enumerate(CATEGORIES):
    folder_path = os.path.join(DATA_DIR, category)
    for file in os.listdir(folder_path)[:300]:  # You can increase this number for more data
        img_path = os.path.join(folder_path, file)
        img = cv2.imread(img_path)
        if img is None:
            continue
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        result = hands.process(img_rgb)
        if result.multi_hand_landmarks:
            hand_landmarks = result.multi_hand_landmarks[0]
            landmark_list = []
            for lm in hand_landmarks.landmark:
                landmark_list.extend([lm.x, lm.y, lm.z])
            data.append(landmark_list)
            labels.append(label_index)

import numpy as np

np.save("X_landmarks.npy", np.array(data))   # Save the landmarks
np.save("y_labels.npy", np.array(labels))    # Save the corresponding labels
print("Saved landmarks and labels to .npy files.")

X = np.load("X_landmarks.npy")
y = np.load("y_labels.npy")
print("Loaded saved landmark data.")

import matplotlib.pyplot as plt

sample_path = os.path.join(DATA_DIR, CATEGORIES[0], os.listdir(os.path.join(DATA_DIR, CATEGORIES[0]))[0])
image = cv2.imread(sample_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

results = hands.process(image_rgb)

if results.multi_hand_landmarks:
    for hand_landmarks in results.multi_hand_landmarks:
        mp_drawing.draw_landmarks(image_rgb, hand_landmarks, mp_hands.HAND_CONNECTIONS)

plt.figure(figsize=(6, 6))
plt.imshow(image_rgb)
plt.title(f"Hand Landmarks - {CATEGORIES[0]}")
plt.axis('off')
plt.show()

data = np.array(data)
labels = np.array(labels)
X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)
y_train_cat = to_categorical(y_train, num_classes=len(CATEGORIES))
y_test_cat = to_categorical(y_test, num_classes=len(CATEGORIES))
print("Data shape:", X_train.shape)

model = models.Sequential([
    layers.Input(shape=(63,)),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(64, activation='relu'),
    layers.Dense(len(CATEGORIES), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

history= model.fit(X_train, y_train_cat, epochs=20, validation_data=(X_test, y_test_cat))

model.save('/content/drive/MyDrive/asl_model_mediapipe.h5')
print("Model saved as asl_mediapipe_model.h5")

import tensorflow as tf

# Load your saved Keras model
model = tf.keras.models.load_model('/content/drive/MyDrive/asl_model_mediapipe.h5')

# Create a TFLiteConverter object from the Keras model
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# (Optional) You can enable optimizations here if needed:
# converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Convert the model
tflite_model = converter.convert()

# Save the TFLite model to your Drive
tflite_model_path = '/content/drive/MyDrive/asl_model_mediapipe.tflite'
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)

print(f"TFLite model saved to {tflite_model_path}")

import matplotlib.pyplot as plt

# Assuming you stored the fit result in a variable called 'history'
# history = model.fit(...)

# Plot Accuracy
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
if 'val_accuracy' in history.history:
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# Initialize MediaPipe Hands with lower detection confidence for testing
hands = mp_hands.Hands(
    static_image_mode=True,
    max_num_hands=1,
    min_detection_confidence=0.3
)

def preprocess_image(image_path):
    """
    Given an image path, read the image, detect hand landmarks using MediaPipe,
    and return a flattened landmarks vector suitable for your model input.
    """
    img = cv2.imread(image_path)
    if img is None:
        raise ValueError("Image not found or unable to read.")

    # Resize for consistency (optional)
    img = cv2.resize(img, (640, 480))

    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    result = hands.process(img_rgb)

    if not result.multi_hand_landmarks:
        print("Warning: No hand detected in the image.")
        return None

    hand_landmarks = result.multi_hand_landmarks[0]

    # Optional: Draw landmarks and show image to verify detection
    annotated_img = img_rgb.copy()
    mp_drawing.draw_landmarks(annotated_img, hand_landmarks, mp_hands.HAND_CONNECTIONS)

    plt.figure(figsize=(6, 6))
    plt.imshow(annotated_img)
    plt.axis('off')
    plt.show()

    landmark_list = []
    for lm in hand_landmarks.landmark:
        landmark_list.extend([lm.x, lm.y, lm.z])

    # Convert to numpy array and reshape for model input
    return np.array(landmark_list).reshape(1, -1)

from google.colab import files

# Upload an image file
uploaded = files.upload()

for filename in uploaded.keys():
    print(f"Processing {filename}...")
    landmarks_vector = preprocess_image(filename)

    if landmarks_vector is None:
        print("Please upload a clear hand gesture image with visible hand.")
        continue

    # Predict using your model
    prediction = model.predict(landmarks_vector)
    predicted_class = np.argmax(prediction)

    print(f"Predicted class index: {predicted_class}")
    print(f"Predicted label: {CATEGORIES[predicted_class]}")  # Ensure CATEGORIES list is loaded/defined